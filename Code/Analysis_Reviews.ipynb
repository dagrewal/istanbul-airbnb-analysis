{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This file contains the code used to analysis reviews for property listings in Istanbul.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import pandas as pd; import numpy as np; import matplotlib.pyplot as plt; import seaborn as sns; import re;\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "from nltk.corpus import stopwords; from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.sparse import coo_matrix\n",
    "from collections import Counter\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "from gensim.models import Word2Vec\n",
    "import multiprocessing\n",
    "from time import time\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in data\n",
    "df = pd.read_csv(\"../Data/DataPrepQ2.csv\", low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view df at high level\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove rows with empty comments\n",
    "df = df.dropna(subset=['comments'], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean the reviewer comments\n",
    "def clean_comment(comment):\n",
    "    \"\"\"\n",
    "    Desc: cleans text in review comments by various means, including removing stopwords, punctuation, numbers and converting to lowercase.\n",
    "    Args: comment -- String\n",
    "    Output: comment_cleaned -- String\n",
    "    \"\"\"\n",
    "    comment = re.sub(r\"[^a-zA-Z ]\", \"\", comment.lower())\n",
    "    comment_tokens = word_tokenize(comment)\n",
    "    stops = set(stopwords.words('english'))\n",
    "    comment_cleaned = ' '.join(word for word in comment_tokens if word not in stops)\n",
    "    return comment_cleaned\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Â clean review comments and store in a list\n",
    "clean_comments = []\n",
    "for i in df.comments:\n",
    "    clean_comments.append(clean_comment(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new feature containing cleaned comments\n",
    "df['comment_cleaned'] = clean_comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join all comments together for wordcloud\n",
    "comments = ' '.join(comment for comment in clean_comments)\n",
    "comments_tokenized = word_tokenize(comments)\n",
    "counter = Counter(comments_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top 10 mos common terms in review comment corpus\n",
    "counter.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# include all terms for wordcloud except istanbul\n",
    "comments = ' '.join(word for word in comments_tokenized if word not in ['istanbul'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create wordcloud and plot\n",
    "wc = WordCloud(background_color='white').generate(comments)\n",
    "plt.figure(figsize=(18,6))\n",
    "plt.imshow(wc, interpolation='bilinear');\n",
    "plt.axis(\"off\");\n",
    "plt.savefig(\"../Static/wc_1.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot wordcloud for kadikoy neighbourhood\n",
    "kadikoy_comments = ' '.join(comment for comment in df.loc[df.neighbourhood_cleansed == 'Kadikoy', 'comments'].tolist())\n",
    "kadikoy_wc = WordCloud(background_color='white').generate(kadikoy_comments)\n",
    "plt.figure(figsize=(18,6))\n",
    "plt.imshow(kadikoy_wc, interpolation='bilinear');\n",
    "plt.axis(\"off\");\n",
    "plt.savefig(\"../Static/wc_2.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot wordcloud for fatih neighbourhood\n",
    "fatih_comments = ' '.join(comment for comment in df.loc[df.neighbourhood_cleansed == 'Fatih', 'comment_cleaned'])\n",
    "fatih_wc = WordCloud(background_color='white').generate(fatih_comments)\n",
    "plt.figure(figsize=(18,6))\n",
    "plt.imshow(fatih_wc, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "plt.savefig(\"../Static/wc_3.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assess comment quality by developing new features\n",
    "df['comment_len'] = df.comment_cleaned.str.split().str.len()\n",
    "df['comment_unique_len'] = df.comment_cleaned.apply(lambda x: len(set(x.split())))\n",
    "df['comment_quality'] = df.comment_unique_len / df.comment_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove comments that had less than 3 words - these won't prove useful for word2vec model\n",
    "df_1 = df.loc[df.comment_len > 3].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert comments to list of lists for preparation for phraser\n",
    "comments = [word_tokenize(comment) for comment in df_1.comment_cleaned]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find phrases in documents\n",
    "phrases = Phrases(comments, min_count=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build bigram phraser\n",
    "bigrams = Phraser(phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform corpus by identifying all relevant bigrams\n",
    "comments_w_bigrams = bigrams[comments]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify number of cores available to train word2vec model\n",
    "cores = multiprocessing.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create w2v model with params initialized\n",
    "#w2v_model = Word2Vec(sg=1, hs=1, min_count=50, window=2, size=300, sample=1e-5, alpha=0.03, min_alpha=0.0006, negative=20, workers=cores-1, iter=3, seed=37)\n",
    "# build the vocab for w2v model\n",
    "#t = time()\n",
    "\n",
    "#w2v_model.build_vocab(comments_w_bigrams, progress_per=10000)\n",
    "\n",
    "#print('Time to build vocab: {} mins'.format(round((time() - t) / 60, 2)))\n",
    "# train the w2v model on the comment data\n",
    "#t = time()\n",
    "\n",
    "#w2v_model.train(comments_w_bigrams, total_examples=w2v_model.corpus_count, epochs=10)\n",
    "\n",
    "#print('Time to train the model: {} mins'.format(round((time() - t) / 60, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the model memory efficient after training\n",
    "#w2v_model.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the pre-trained w2v model\n",
    "w2v_model = Word2Vec.load(\"../Models/word_2_vec_comments.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the results of the model\n",
    "w2v_model.wv.most_similar(positive=['great'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.wv.most_similar(positive=['ferry'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.wv.most_similar(positive=[\"culture\", \"food\"], negative=[\"location\"], topn=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# persist the w2v model\n",
    "#w2v_model.save(\"../Models/word_2_vec_comments.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply kmeans model to word vectors to identify langauge used to write reviews\n",
    "kmeans_model = KMeans(n_clusters=4, n_init=20, max_iter=1000, random_state=37)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit kmeans model\n",
    "cluster_comments = kmeans_model.fit(w2v_model.wv.vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view results of kmeans \n",
    "w2v_model.wv.similar_by_vector(kmeans_model.cluster_centers_[3], topn=10, restrict_vocab=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign labels to words - first create new df for terms, vectors and cluster assignments\n",
    "terms_df = pd.DataFrame(w2v_model.wv.vocab.keys())\n",
    "terms_df.columns = ['term']\n",
    "terms_df['vector'] = terms_df.term.apply(lambda x: w2v_model.wv[f\"{x}\"])\n",
    "terms_df['cluster'] = terms_df.vector.apply(lambda x: kmeans_model.predict([np.array(x)]))\n",
    "terms_df.cluster = terms_df.cluster.apply(lambda x: x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter english terms\n",
    "english_terms = set(terms_df.loc[terms_df.cluster == 1, 'term'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make new df showing comments\n",
    "df_1['comment'] = [i for i in comments_w_bigrams]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new feature showing number of words in comment that are from english terms\n",
    "df_1['language'] = df_1.comment.apply(lambda x: len([word for word in x if word in english_terms]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new df for english comments\n",
    "english_df = df_1.loc[df_1.language > 0].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine if comment has enough english words to be considered english\n",
    "english_df.language = english_df.apply(lambda x: 'English' if x['language'] / len(x['comment']) >= .5 else 'Foreign', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter on language column\n",
    "english_df = english_df.loc[english_df.language == 'English']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "w2v_model.wv.most_similar(\"turkish\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def restrict_w2v(w2v, restricted_word_set):\n",
    "    \"\"\"\n",
    "    Desc: Restrict w2v model to a subset of terms. E.g. Restrict w2v model to only English terms. Function has been adapted from\n",
    "          https://stackoverflow.com/questions/48941648/how-to-remove-a-word-completely-from-a-word2vec-model-in-gensim\n",
    "    Args: w2v -- Gensim Word2Vec model\n",
    "          restricted_word_set -- set or list\n",
    "    Output: w2v -- Restricted Word2Vec model\n",
    "    \"\"\"\n",
    "    new_vectors = []\n",
    "    new_vocab = {}\n",
    "    new_index2entity = []\n",
    "    new_vectors_norm = []\n",
    "\n",
    "    for i in range(len(w2v.wv.vocab)):\n",
    "        word = w2v.wv.index2entity[i]\n",
    "        vec = w2v.wv.vectors[i]\n",
    "        vocab = w2v.wv.vocab[word]\n",
    "        vec_norm = w2v.wv.vectors_norm[i]\n",
    "        if word in restricted_word_set:\n",
    "            vocab.index = len(new_index2entity)\n",
    "            new_index2entity.append(word)\n",
    "            new_vocab[word] = vocab\n",
    "            new_vectors.append(vec)\n",
    "            new_vectors_norm.append(vec_norm)\n",
    "\n",
    "    w2v.wv.vocab = new_vocab\n",
    "    w2v.wv.vectors = np.array(new_vectors)\n",
    "    w2v.wv.index2entity = np.array(new_index2entity)\n",
    "    w2v.wv.index2word = np.array(new_index2entity)\n",
    "    w2v.wv.vectors_norm = np.array(new_vectors_norm)\n",
    "    \n",
    "    return w2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Â create english w2v model\n",
    "english_w2v = restrict_w2v(w2v_model, english_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create kmeans on english comments\n",
    "english_kmeans = KMeans(n_clusters=13, n_init=20, max_iter=1000, random_state=37)\n",
    "cluster_english = english_kmeans.fit(english_w2v.wv.vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check results of model\n",
    "english_w2v.wv.similar_by_vector(english_kmeans.cluster_centers_[7], topn=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_w2v.wv.similar_by_vector(english_kmeans.cluster_centers_[12], topn=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Â create names for topics\n",
    "topics_map = {0:'authentic, charming, beautiful neighbourhood', 1:'helpful host', 2: 'general host', 3: 'difficulties and things to be aware', 4: 'negatives',\n",
    "             5: 'positive stay', 6: 'all about the views', 7: 'transport', 8: 'accommodation essentials', 9: 'proximity to tourist stuff', 10: 'good location',\n",
    "             11:'turkish names', 12: 'food and drink'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Â create new df for clusters of english terms\n",
    "english_terms_df = pd.DataFrame(english_w2v.wv.vocab.keys())\n",
    "english_terms_df.columns = ['terms']\n",
    "english_terms_df['vectors'] = english_terms_df.terms.apply(lambda x: english_w2v.wv[f'{x}'])\n",
    "english_terms_df['cluster'] = english_terms_df.vectors.apply(lambda x: english_kmeans.predict([np.array(x)]))\n",
    "english_terms_df.cluster = english_terms_df.cluster.apply(lambda x: x[0])\n",
    "english_terms_df.cluster = english_terms_df.cluster.map(topics_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dictionary of terms and clusters\n",
    "term_clusters = dict(zip(english_terms_df['terms'], english_terms_df['cluster']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create list of english terms to keep\n",
    "term_keys = set(term_clusters.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each comment, remove terms not recognised in english terms dictionary\n",
    "for i in english_df.comment:\n",
    "    for j in i:\n",
    "        if j not in term_keys:\n",
    "            i.remove(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the number of unique terms\n",
    "max_features = pd.Series(' '.join(i for i in english_df.comment.str.join(' ')).split()).nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create tfidf model \n",
    "tfidf_vect = TfidfVectorizer(ngram_range=(1,1), norm='l2', sublinear_tf=True, smooth_idf=True, min_df=1, max_df=1., max_features=max_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit and transform tfidf model to english comments\n",
    "tfidf_comments = tfidf_vect.fit_transform(english_df.comment.str.join(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the feature names from tfidf\n",
    "comment_features = tfidf_vect.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_coo(coo_matrix):\n",
    "    \"\"\"\n",
    "    Desc: For each tfidf comment, store terms sorted based on their tfidf score. Function has been taken from\n",
    "          https://medium.com/analytics-vidhya/automated-keyword-extraction-from-articles-using-nlp-bfd864f41b34\n",
    "    Args: coo_matrix -- SparseMatrix\n",
    "    Output: tuples -- tuple\n",
    "    \"\"\"\n",
    "    tuples = zip(coo_matrix.col, coo_matrix.data)\n",
    "    return sorted(tuples, key=lambda x: (x[1], x[0]), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_topn_from_vector(feature_names, sorted_items, english_terms, topn=10):\n",
    "    \"\"\"\n",
    "    Desc: Extact the top terms based on tfidf score from each comment, along wth their cluster assignment and tfidf score. Function adapted from\n",
    "          https://medium.com/analytics-vidhya/automated-keyword-extraction-from-articles-using-nlp-bfd864f41b34\n",
    "    Args: feature_names -- list or set -- tfidf features\n",
    "          sorted_items -- tuple -- tfidf comment indexes sorted by tfidf score\n",
    "          english_terms -- set or list -- collection of terms considered as English\n",
    "          topn: int -- number of top terms to return\n",
    "    Output: results -- dict -- collection of top terms and their cluster assignment\n",
    "    \"\"\"\n",
    "    \n",
    "    #use only topn items from vector\n",
    "    sorted_items = sorted_items[:topn]\n",
    " \n",
    "    score_vals = []\n",
    "    feature_vals = []\n",
    "    cluster_vals = []\n",
    "    \n",
    "    # word index and corresponding tf-idf score\n",
    "    for idx, score in sorted_items:\n",
    "        \n",
    "        #keep track of feature name and its corresponding score\n",
    "        term = feature_names[idx]\n",
    "        if term in english_terms:\n",
    "            score_vals.append(round(score, 3))\n",
    "            feature_vals.append(term)\n",
    "            cluster_vals.append(term_clusters[term])\n",
    "        \n",
    " \n",
    "    #create a tuples of feature,score\n",
    "    #results = zip(feature_vals,score_vals)\n",
    "    results= {}\n",
    "    for idx in range(len(feature_vals)):\n",
    "        results[feature_vals[idx]]=[score_vals[idx], cluster_vals[idx]]\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain tfidf comments sorted by term tfidf scores and store in list\n",
    "sorted_items_list = []\n",
    "for i in tfidf_comments:\n",
    "    sorted_items_list.append(sort_coo(i.tocoo()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain top terms per comment and store in list\n",
    "keywords_list = []\n",
    "for i in sorted_items_list:\n",
    "    keywords_list.append(extract_topn_from_vector(comment_features, i, english_terms, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract cluster assigned to top terms from each comment\n",
    "themes = []\n",
    "for i in keywords_list:\n",
    "    themes.append([j[1] for j in i.values()])\n",
    "\n",
    "themes = [list(set(i)) for i in themes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add themes of comment to df\n",
    "english_df['comment_themes'] = themes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join cluster names together into a string\n",
    "english_df.comment_themes = english_df.comment_themes.str.join(', ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove redundant features\n",
    "english_df = english_df.drop(['id_x', 'id_y'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset index\n",
    "english_df = english_df.reset_index(drop=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
